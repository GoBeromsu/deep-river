{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Mini Batches\n",
    "Iterate over a data stream in mini batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from river import datasets, compose, metrics, preprocessing\n",
    "from torch import nn\n",
    "from river_torch import classification\n",
    "from itertools import islice"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "dataset = datasets.Phishing()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.dense0 = nn.Linear(n_features,5)\n",
    "        self.nonlin = nn.ReLU()\n",
    "        self.dense1 = nn.Linear(5, 2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = self.softmax(X)\n",
    "        return X\n",
    "\n",
    "def batcher(iterable, batch_size):\n",
    "    iterator = iter(iterable)\n",
    "    while batch := list(islice(iterator, batch_size)):\n",
    "        yield batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  Classifier (\n    module=<class '__main__.MyModule'>\n    loss_fn=\"binary_cross_entropy\"\n    optimizer_fn=<class 'torch.optim.sgd.SGD'>\n    lr=0.001\n    device=\"cpu\"\n    seed=42\n  )\n)",
      "text/html": "<div><div class=\"river-component river-pipeline\"><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">StandardScaler</pre></summary><code class=\"river-estimator-params\">\n{'counts': Counter(),\n 'means': defaultdict(&lt;class 'float'&gt;, {}),\n 'vars': defaultdict(&lt;class 'float'&gt;, {}),\n 'with_std': True}\n\n</code></details><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">Classifier</pre></summary><code class=\"river-estimator-params\">\n{'device': 'cpu',\n 'kwargs': {},\n 'loss_fn': &lt;function binary_cross_entropy at 0x0000020CD030F310&gt;,\n 'lr': 0.001,\n 'module': &lt;class '__main__.MyModule'&gt;,\n 'module_initialized': False,\n 'observed_classes': [],\n 'optimizer_fn': &lt;class 'torch.optim.sgd.SGD'&gt;,\n 'output_layer': None,\n 'seed': 42}\n\n</code></details></div><style scoped>\n.river-estimator {\n    padding: 1em;\n    border-style: solid;\n    background: white;\n}\n\n.river-pipeline {\n    display: flex;\n    flex-direction: column;\n    align-items: center;\n    background: linear-gradient(#000, #000) no-repeat center / 3px 100%;\n}\n\n.river-union {\n    display: flex;\n    flex-direction: row;\n    align-items: center;\n    justify-content: center;\n    padding: 1em;\n    border-style: solid;\n    background: white\n}\n\n.river-wrapper {\n    display: flex;\n    flex-direction: column;\n    align-items: center;\n    justify-content: center;\n    padding: 1em;\n    border-style: solid;\n    background: white;\n}\n\n.river-wrapper > .river-estimator {\n    margin-top: 1em;\n}\n\n/* Vertical spacing between steps */\n\n.river-component + .river-component {\n    margin-top: 2em;\n}\n\n.river-union > .river-estimator {\n    margin-top: 0;\n}\n\n.river-union > .pipeline {\n    margin-top: 0;\n}\n\n/* Spacing within a union of estimators */\n\n.river-union > .river-component + .river-component {\n    margin-left: 1em;\n}\n\n/* Typography */\n\n.river-estimator-params {\n    display: block;\n    white-space: pre-wrap;\n    font-size: 120%;\n    margin-bottom: -1em;\n}\n\n.river-estimator > .river-estimator-params,\n.river-wrapper > .river-details > river-estimator-params {\n    background-color: white !important;\n}\n\n.river-estimator-name {\n    display: inline;\n    margin: 0;\n    font-size: 130%;\n}\n\n/* Toggle */\n\n.river-summary {\n    display: flex;\n    align-items:center;\n    cursor: pointer;\n}\n\n.river-summary > div {\n    width: 100%;\n}\n</style></div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipeline = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    classification.Classifier(module=MyModule,loss_fn=\"binary_cross_entropy\",optimizer_fn=\"sgd\")\n",
    ")\n",
    "model_pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'out_features'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(y)\n\u001B[0;32m      5\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model_pipeline\u001B[38;5;241m.\u001B[39mpredict_proba_many(X\u001B[38;5;241m=\u001B[39mx)\n\u001B[1;32m----> 6\u001B[0m model_pipeline \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_pipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn_many\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\environments\\river-torch\\lib\\site-packages\\river\\compose\\pipeline.py:760\u001B[0m, in \u001B[0;36mPipeline.learn_many\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m    758\u001B[0m last_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(steps)\n\u001B[0;32m    759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m last_step\u001B[38;5;241m.\u001B[39m_supervised:\n\u001B[1;32m--> 760\u001B[0m     last_step\u001B[38;5;241m.\u001B[39mlearn_many(X\u001B[38;5;241m=\u001B[39mX, y\u001B[38;5;241m=\u001B[39my, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m    761\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    762\u001B[0m     last_step\u001B[38;5;241m.\u001B[39mlearn_many(X\u001B[38;5;241m=\u001B[39mX, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n",
      "File \u001B[1;32m~\\Documents\\projects\\IncrementalLearning\\river-torch\\river_torch\\classification\\classifier.py:251\u001B[0m, in \u001B[0;36mClassifier.learn_many\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    245\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m y_i \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobserved_classes:\n\u001B[0;32m    246\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobserved_classes\u001B[38;5;241m.\u001B[39mappend(y_i)\n\u001B[0;32m    248\u001B[0m y \u001B[38;5;241m=\u001B[39m labels2onehot(\n\u001B[0;32m    249\u001B[0m     y,\n\u001B[0;32m    250\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobserved_classes,\n\u001B[1;32m--> 251\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_layer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_features\u001B[49m,\n\u001B[0;32m    252\u001B[0m     device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice,\n\u001B[0;32m    253\u001B[0m )\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_learn(x\u001B[38;5;241m=\u001B[39mX, y\u001B[38;5;241m=\u001B[39my)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'out_features'"
     ]
    }
   ],
   "source": [
    "for batch in batcher(dataset,5):\n",
    "    x,y = zip(*batch)\n",
    "    x = pd.DataFrame(x)\n",
    "    y = list(y)\n",
    "    y_pred = model_pipeline.predict_proba_many(X=x)\n",
    "    model_pipeline = model_pipeline.learn_many(x, y)    # make the model learn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}